{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \nimport matplotlib.pyplot as plt\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": false,
        "_uuid": "0e6d604896c5e457132392cb071733a1a52ec08a"
      },
      "cell_type": "code",
      "source": "raw_data = pd.read_csv(\"../input/train_data.csv\")\n\nprint(raw_data.head())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1fa5f04dae9d1afd85f3b29ff7d9d966f06c9184"
      },
      "cell_type": "code",
      "source": "from sklearn import preprocessing\n# Preprocess\n# feature_head = ['MidPrice','LastPrice','Volume','BidPrice1','BidVolume1','AskPrice1','AskVolume1']\nfeature_head = ['MidPrice','LastPrice','BidVolume1', \"AskVolume1\"]\nprice_head = ['MidPrice','LastPrice','BidPrice1','AskPrice1']\n\ndef get_parse_am_pm(x_date):\n    x_df = x_date[1]\n    x_df['Hour'] = x_df['Time'].apply(lambda x : int(x[:2]))\n    x_am = x_df[x_df['Hour'] <=11]\n    x_pm = x_df[x_df['Hour']>11]\n    return x_am,x_pm\n   \ndef get_train_data(x_date):\n    x_am,x_pm = get_parse_am_pm(x_date)\n    return x_am,x_pm\n\n                        \ndef prepro(x_data):\n    x_dataset = []\n    y_dataset = []\n    for i in range(int(len(x_data)/10)-2):\n        standard_price = x_data['MidPrice'].iloc[i*10+9]\n\n        x_data_feature = x_data[feature_head].iloc[i*10:i*10+10]\n#         x_data_feature['MidPrice'] = x_data_feature['MidPrice'] - standard_price\n        x_data_feature = np.array(x_data_feature).tolist()\n#         print(x_data_feature)\n        y_item = sum(x_data['MidPrice'].iloc[i*10+10:i*10+30])/20 - standard_price\n        \n        x_dataset.append(x_data_feature)\n        y_dataset.append(y_item)\n    return x_dataset, y_dataset\n\n\n# X_train = pd.DataFrame(columns=feature_head)\n# Y_train = pd.DataFrame()\nX_train = []\nY_train = []\nX_date = raw_data.groupby('Date')\nfor x_date in X_date:\n    # Extract am/pm\n    print(x_date[0])\n    x_train_am,x_train_pm = get_train_data(x_date)\n    x_train_am.sample(frac=1)\n    x_train_pm.sample(frac=1)\n    \n    # Preprocess\n    x_data_am, y_data_am = prepro(x_train_am)\n    x_data_pm, y_data_pm = prepro(x_train_pm)\n    X_train = X_train + x_data_am + x_data_pm\n    Y_train = Y_train + y_data_am + y_data_pm\n\nprint(len(X_train))\nprint(len(Y_train))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "10149f95b380baefc07dad1c482fd274897e7b96"
      },
      "cell_type": "code",
      "source": "def normalize(x_train_set):\n    #Normalize\n    for i in range(x_train_set.shape[0]):\n        minmaxscaler = preprocessing.MinMaxScaler().fit(x_train_set[i,:,3:4])\n        x_train_set[i,:,3:4] = minmaxscaler.transform(x_train_set[i,:,3:4])\n\n        minmaxscaler = preprocessing.MinMaxScaler().fit(x_train_set[i,:,2:3])\n        x_train_set[i,:,2:3] = minmaxscaler.transform(x_train_set[i,:,2:3])\n\n#     x_train_set[:,:,3] = preprocessing.scale(x_train_set[:,:,3])\n#     x_train_set[:,:,2] = preprocessing.scale(x_train_set[:,:,2])\n#     x_train_set[:,:,0] = preprocessing.scale(x_train_set[:,0])\n    for i in range(x_train_set.shape[0]):\n        standard_price = x_train_set[i,-1,0]\n        x_train_set[i,:,0] = (x_train_set[i,:,0] - standard_price) * 1000 \n        x_train_set[i,:,1] = (x_train_set[i,:,1] - standard_price) * 1000\n#     x_train_set[:,:,0] = \n    return x_train_set\n\nx_train_set = np.array(X_train)\ny_train_set = np.array(Y_train)\n# x_train_set = x_train_set[:,:]\n# y_train_set = y_train_set[:,1]\nprint(x_train_set.shape)\nprint(y_train_set.shape)\n\nx_train_set = normalize(x_train_set)\n# print(x_train_set[0:3])\n\ny_train_set[:] = y_train_set[:]*1000\ny_train_set = np.reshape(y_train_set,(y_train_set.shape[0],1))\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8e262456792bfefe22cfebb4286064eeddc6ada2"
      },
      "cell_type": "code",
      "source": "from keras.layers import Activation, Dense, LSTM, Dropout\nfrom keras.models import Sequential\n\nactiv_func = 'tanh'\n\nneurons = 128\nloss = 'mse'                  \noptimizer=\"adam\"              \ndropout = 0.2      \n# batch_size = 12               \n\n# window_len = 7\n\nmodel = Sequential()\nmodel.add(Dense(neurons, input_shape=(x_train_set.shape[1], x_train_set.shape[2]),))\nmodel.add(LSTM(neurons, return_sequences=True,  activation=activ_func))\nmodel.add(Dropout(dropout))\nmodel.add(LSTM(neurons, return_sequences=False, activation=activ_func))\n# model.add(Dropout(dropout))\n# model.add(LSTM(neurons, activation=activ_func))\n# model.add(Dropout(dropout))\nmodel.add(Dense(24, activation=activ_func))\nmodel.add(Dense(1,activation=activ_func))\nmodel.add(Activation(activ_func))\nmodel.compile(loss=loss, optimizer=optimizer, metrics=['mse'])\nmodel.summary()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "5978125b36b81048877568c6ca71397c68b68fd0"
      },
      "cell_type": "code",
      "source": "from keras.callbacks import EarlyStopping \n\nepochs = 20\n\n# early_stopping = EarlyStopping(monitor='loss',patience=2, verbose=0, mode='auto')\nhistory = model.fit(x_train_set,y_train_set,epochs = epochs,batch_size = 32)\n# history = model.fit(x_train_set,y_train_set,epochs = epochs,batch_size = 32,callbacks=[early_stopping])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "057f6e951cf60c651e121ca1a5f89e0d76a873af"
      },
      "cell_type": "code",
      "source": "from sklearn import preprocessing\ndef normalize(sss):\n    #Normalize\n    \n    minmaxscaler = preprocessing.MinMaxScaler().fit(sss[0,:,3:4])\n    sss[0,:,3:4] = minmaxscaler.transform(sss[0,:,3:4])\n    \n    minmaxscaler = preprocessing.MinMaxScaler().fit(sss[0,:,2:3])\n    sss[0,:,2:3] = minmaxscaler.transform(sss[0,:,2:3])\n    \n    for i in range(sss.shape[0]):\n        standard_price = sss[i,-1,0]\n        sss[i,:,0] = (sss[i,:,0] - standard_price) * 1000\n        sss[i,:,1] = (sss[i,:,1] - standard_price) * 1000\n#         minmaxscaler = preprocessing.MinMaxScaler().fit(x_train_set[i:i+1,:,0])\n#         x_train_set[i:i+1,:,0] = minmaxscaler.transform(x_train_set[i:i+1,:,0])\n#     x_train_set[:,:,0] = \n    return sss\n\ntest_data = pd.read_csv(\"../input/test_data.csv\")\nprint(len(test_data))\n# print(test_data[feature_head].loc[0:10])\nans_list = []\nans_for_hist = []\nans_csv = pd.DataFrame(columns=['caseid','midprice'])\n\nfor i in range(142,1000):\n    standard_price = test_data['MidPrice'].iloc[i*10+9]\n\n    X_test = test_data[feature_head].iloc[i*10:i*10+10]\n    X_test = np.array(X_test)\n    X_test = X_test.reshape(1,X_test.shape[0],X_test.shape[1])\n    \n    X_test = normalize(X_test)\n#     print(X_test)\n    #     X_test = np.array(X_test)\n#     print(X_test.shape)\n    Y_test = model.predict(X_test)\n#     print(Y_test)\n    ans = standard_price + Y_test[0,0]/1000\n    \n    ans_for_hist = ans_for_hist + [Y_test[0,0]/1000]\n#     ans = standard_price + sum(Y_test[:,0,0])/len(Y_test[:,0,0])\n    \n    ans_list = ans_list + [ ans ]\n    ans_csv.loc[i-142]=[i+1,ans]\n# plt.hist(ans)\nplt.hist(ans_for_hist)\n# print(ans)\nans_csv.to_csv('data.csv', index = False, index_label = False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2de65c86ab007e00713b63a69f47c08f7c60e10f"
      },
      "cell_type": "code",
      "source": "ans_list",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4b49ad917a10e061ed6f747479fd5d417514f1a7"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}